<!DOCTYPE html>
<html lang="en-us">
  <head>
    <meta charset="UTF-8">
    <title>Machine Learning Project by ermckinnon</title>
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <link rel="stylesheet" type="text/css" href="stylesheets/normalize.css" media="screen">
    <link href='https://fonts.googleapis.com/css?family=Open+Sans:400,700' rel='stylesheet' type='text/css'>
    <link rel="stylesheet" type="text/css" href="stylesheets/stylesheet.css" media="screen">
    <link rel="stylesheet" type="text/css" href="stylesheets/github-light.css" media="screen">
  </head>
  <body>
    <section class="page-header">
      <h1 class="project-name">Machine Learning Project</h1>
      <h2 class="project-tagline">Final Project Report</h2>
      <a href="https://github.com/ermckinnon/github.io" class="btn">View on GitHub</a>
      <a href="https://github.com/ermckinnon/github.io/zipball/master" class="btn">Download .zip</a>
      <a href="https://github.com/ermckinnon/github.io/tarball/master" class="btn">Download .tar.gz</a>
    </section>

    <section class="main-content">
      

<p></p>

<p></p>

<p></p>

<p></p>

<p></p>Machine Learning Project

<p></p><div id="MathJax_Message"></div>

<div id="header">
<h1>
<a id="machine-learning-project" class="anchor" href="#machine-learning-project" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Machine Learning Project</h1>
<h4>
<a id="ewen-mckinnon" class="anchor" href="#ewen-mckinnon" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a><em>Ewen McKinnon</em>
</h4>
<h4>
<a id="sunday-may-15-2016" class="anchor" href="#sunday-may-15-2016" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a><em>Sunday, May 15, 2016</em>
</h4>
</div>

<div id="background">
<h1>
<a id="background" class="anchor" href="#background" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Background</h1>
<p>This project uses data from accelerometers on the belt, forearm, arm, and dumbbell of 6 participants in a physical exercise trial. Participants were asked to perform dumbbell lifts correctly and incorrectly in 5 different ways. The aim of this report is to see if we can accurately classify incorrect and correct performance from the accelerometer data - and then to run the final classifier on a set of 20 test questions.</p>
</div>

<div id="methodology">
<h1>
<a id="methodology" class="anchor" href="#methodology" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Methodology</h1>
<p>Given a very clear analytical question my methodological approach for this project was to:<br>* load and clean the data to reduce the data to useful features only<br>* split the data into training and testing datasets<br>* explore the potential features to determine if any pre-processing is required<br>* run some candidate classifiers - classification tree, random forest and gradient boosting<br>* perform cross-validation on test sets<br>* explore the possibility of blending classifiers to improve performance<br>* select a final classifier and run it on the project test questions</p>
</div>

<div id="data-cleansing-and-preparation">
<h1>
<a id="data-cleansing-and-preparation" class="anchor" href="#data-cleansing-and-preparation" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Data Cleansing and Preparation</h1>
<p>The data can be downloaded from <a href="http://groupware.les.inf.puc-rio.br/har">http://groupware.les.inf.puc-rio.br/har</a>. In the training data there are 159 variables with an additional single classification variable ‘classe’. However many of the variables in the training data are dominated by missing data. Furthermore, these variables are entirely missing in the final test question dataset and they needed to be removed. In addition, there are time stamp variables which intuitively should not contribute as features within the classifier and I have removed these too. Annex A provides a summary of the 54 useful variables I retained for classification analysis.</p>
<p>I have split the data into training and testing dataset on a 75:25 basis. So in total there are 14718 observations in the training dataset and 4904 observations in the testing dataset for cross-validation.</p>
</div>

<div id="exploratory-analysis">
<h1>
<a id="exploratory-analysis" class="anchor" href="#exploratory-analysis" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Exploratory Analysis</h1>
<p>Exploratory analysis highlights non-normal distributions among the potential classifier features. The charts below illustrate a few histograms of features and show skewed and bi-modal distributions. These suggest pre-processing using centering and scaling might help with some classifications algorithms - particularly generalised linear regression modelling.</p>
<p><img title="" alt="" width="672"></p>
</div>

<div id="development-of-classifiers-and-cross-validation-results">
<h1>
<a id="development-of-classifiers-and-cross-validation-results" class="anchor" href="#development-of-classifiers-and-cross-validation-results" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Development of Classifiers and Cross-validation Results</h1>
<p>The classification variable consists of a factor variable with 5 levels - in other words one correct form of exercise and four incorrect. In practice this means binomial logistic regression is not possible because there are more than two classification possibilities. I therefore ran three types of models - classification tree, boosting and random forest, with two forms of pre-processing - centre &amp; scaling and Principle Components Analysis.</p>
<p>The two best models which emerged from this analysis were Random Forest and Boosting, without any pre-processing. Centre and scaling, and PCA pre-processing in almost all cases produced poorer classification results. Classification tree algorithms performed particularly poorly and do not appear suited to this dataset.</p>
<p>Confusion Matrices for the two best models are presented in Annexes B and C. However the headline performance and error rates are as follows:</p>
<ul>
<li>Random Forest in sample accuracy 99.98% with error rate 0.02%<br>
</li>
<li>Random Forest out of sample accuracy 99.96% with error rate 0.04%<br>
</li>
<li>Boosting in sample accuracy 99.21% with error rate 0.79%<br>
</li>
<li>Boosting out of sample sample accuracy 99.29% with error rate 0.71%</li>
</ul>
<p>So both classifiers perform well but with the the Random Forest classifier performing marginally better.</p>
<div id="discussion-on-combinational-ensemble-classification">
<h2>
<a id="discussion-on-combinational-ensemble-classification" class="anchor" href="#discussion-on-combinational-ensemble-classification" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Discussion on Combinational/ Ensemble Classification</h2>
<p>There is clearly very little room for improvement given the high accuracy of the classifiers however it is possible to construct a majority voting combination of the two classifiers. On the training data set the two classifiers agree with each other on 99.23% of observations. On the testing dataset they agree on 99.33% of observations. So some dissagrement suggests there is a potential for a very small improvement using a combination classifier. Such small improvements in accuracy might help with thousands of classifications, however over a small set of test questions there will be little to gain from developing a combination classifier - in fact both classifiers give exactly the same results on the final test questions. Furthermore given the very high processing times for the two classifiers (both took take 3hrs to run on my computer) I would recommend using the random forest classifier which its slightly higher accuracy and almost halving the computation time that would be associated with tuning a combinational classifier.</p>
</div>

<p></p>
</div>

<div id="final-classifier-results">
<h1>
<a id="final-classifier-results" class="anchor" href="#final-classifier-results" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Final Classifier Results</h1>
<p>The table below provides the final results against the 20 quiz questions using the random forest classifier which has a measured out of sample accuracy of 99.96% and out of sample error rate of 0.04%.</p>
<pre><code>##    problem_id rforrest_prediction
## 1           1                   B
## 2           2                   A
## 3           3                   B
## 4           4                   A
## 5           5                   A
## 6           6                   E
## 7           7                   D
## 8           8                   B
## 9           9                   A
## 10         10                   A
## 11         11                   B
## 12         12                   C
## 13         13                   B
## 14         14                   A
## 15         15                   E
## 16         16                   E
## 17         17                   A
## 18         18                   B
## 19         19                   B
## 20         20                   B</code></pre>
</div>

<div id="annex-a-summary-of-variables-used-in-classification-training-set">
<h1>
<a id="annex-a-summary-of-variables-used-in-classification-training-set" class="anchor" href="#annex-a-summary-of-variables-used-in-classification-training-set" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Annex A Summary of variables used in classification training set</h1>
<pre><code>## 'data.frame':    14718 obs. of  55 variables:
##  $ user_name           : Factor w/ 6 levels "adelmo","carlitos",..: 2 2 2 2 2 2 2 2 2 2 ...
##  $ num_window          : int  11 12 12 12 12 12 12 12 12 12 ...
##  $ roll_belt           : num  1.41 1.48 1.48 1.45 1.42 1.42 1.45 1.45 1.43 1.42 ...
##  $ pitch_belt          : num  8.07 8.05 8.07 8.06 8.09 8.13 8.17 8.18 8.18 8.2 ...
##  $ yaw_belt            : num  -94.4 -94.4 -94.4 -94.4 -94.4 -94.4 -94.4 -94.4 -94.4 -94.4 ...
##  $ total_accel_belt    : int  3 3 3 3 3 3 3 3 3 3 ...
##  $ gyros_belt_x        : num  0.02 0.02 0.02 0.02 0.02 0.02 0.03 0.03 0.02 0.02 ...
##  $ gyros_belt_y        : num  0 0 0.02 0 0 0 0 0 0 0 ...
##  $ gyros_belt_z        : num  -0.02 -0.03 -0.02 -0.02 -0.02 -0.02 0 -0.02 -0.02 0 ...
##  $ accel_belt_x        : int  -22 -22 -21 -21 -22 -22 -21 -21 -22 -22 ...
##  $ accel_belt_y        : int  4 3 2 4 3 4 4 2 2 4 ...
##  $ accel_belt_z        : int  22 21 24 21 21 21 22 23 23 21 ...
##  $ magnet_belt_x       : int  -7 -6 -6 0 -4 -2 -3 -5 -2 -3 ...
##  $ magnet_belt_y       : int  608 604 600 603 599 603 609 596 602 606 ...
##  $ magnet_belt_z       : int  -311 -310 -302 -312 -311 -313 -308 -317 -319 -309 ...
##  $ roll_arm            : num  -128 -128 -128 -128 -128 -128 -128 -128 -128 -128 ...
##  $ pitch_arm           : num  22.5 22.1 22.1 22 21.9 21.8 21.6 21.5 21.5 21.4 ...
##  $ yaw_arm             : num  -161 -161 -161 -161 -161 -161 -161 -161 -161 -161 ...
##  $ total_accel_arm     : int  34 34 34 34 34 34 34 34 34 34 ...
##  $ gyros_arm_x         : num  0.02 0.02 0 0.02 0 0.02 0.02 0.02 0.02 0.02 ...
##  $ gyros_arm_y         : num  -0.02 -0.03 -0.03 -0.03 -0.03 -0.02 -0.03 -0.03 -0.03 -0.02 ...
##  $ gyros_arm_z         : num  -0.02 0.02 0 0 0 0 -0.02 0 0 -0.02 ...
##  $ accel_arm_x         : int  -290 -289 -289 -289 -289 -289 -288 -290 -288 -287 ...
##  $ accel_arm_y         : int  110 111 111 111 111 111 110 110 111 111 ...
##  $ accel_arm_z         : int  -125 -123 -123 -122 -125 -124 -124 -123 -123 -124 ...
##  $ magnet_arm_x        : int  -369 -372 -374 -369 -373 -372 -376 -366 -363 -372 ...
##  $ magnet_arm_y        : int  337 344 337 342 336 338 334 339 343 338 ...
##  $ magnet_arm_z        : int  513 512 506 513 509 510 516 509 520 509 ...
##  $ roll_dumbbell       : num  13.1 13.4 13.4 13.4 13.1 ...
##  $ pitch_dumbbell      : num  -70.6 -70.4 -70.4 -70.8 -70.2 ...
##  $ yaw_dumbbell        : num  -84.7 -84.9 -84.9 -84.5 -85.1 ...
##  $ total_accel_dumbbell: int  37 37 37 37 37 37 37 37 37 37 ...
##  $ gyros_dumbbell_x    : num  0 0 0 0 0 0 0 0 0 0 ...
##  $ gyros_dumbbell_y    : num  -0.02 -0.02 -0.02 -0.02 -0.02 -0.02 -0.02 -0.02 -0.02 -0.02 ...
##  $ gyros_dumbbell_z    : num  0 -0.02 0 0 0 0 0 0 0 -0.02 ...
##  $ accel_dumbbell_x    : int  -233 -232 -233 -234 -232 -234 -235 -233 -233 -234 ...
##  $ accel_dumbbell_y    : int  47 48 48 48 47 46 48 47 47 48 ...
##  $ accel_dumbbell_z    : int  -269 -269 -270 -269 -270 -272 -270 -269 -270 -269 ...
##  $ magnet_dumbbell_x   : int  -555 -552 -554 -558 -551 -555 -558 -564 -554 -552 ...
##  $ magnet_dumbbell_y   : int  296 303 292 294 295 300 291 299 291 302 ...
##  $ magnet_dumbbell_z   : num  -64 -60 -68 -66 -70 -74 -69 -64 -65 -69 ...
##  $ roll_forearm        : num  28.3 28.1 28 27.9 27.9 27.8 27.7 27.6 27.5 27.2 ...
##  $ pitch_forearm       : num  -63.9 -63.9 -63.9 -63.9 -63.9 -63.8 -63.8 -63.8 -63.8 -63.9 ...
##  $ yaw_forearm         : num  -153 -152 -152 -152 -152 -152 -152 -152 -152 -151 ...
##  $ total_accel_forearm : int  36 36 36 36 36 36 36 36 36 36 ...
##  $ gyros_forearm_x     : num  0.02 0.02 0.02 0.02 0.02 0.02 0.02 0.02 0.02 0 ...
##  $ gyros_forearm_y     : num  0 -0.02 0 -0.02 0 -0.02 0 -0.02 0.02 0 ...
##  $ gyros_forearm_z     : num  -0.02 0 -0.02 -0.03 -0.02 0 -0.02 -0.02 -0.03 -0.03 ...
##  $ accel_forearm_x     : int  192 189 189 193 195 193 190 193 191 193 ...
##  $ accel_forearm_y     : int  203 206 206 203 205 205 205 205 203 205 ...
##  $ accel_forearm_z     : int  -216 -214 -214 -215 -215 -213 -215 -214 -215 -215 ...
##  $ magnet_forearm_x    : int  -18 -16 -17 -9 -18 -9 -22 -17 -11 -15 ...
##  $ magnet_forearm_y    : num  661 658 655 660 659 660 656 657 657 655 ...
##  $ magnet_forearm_z    : num  473 469 473 478 470 474 473 465 478 472 ...
##  $ classe              : Factor w/ 5 levels "A","B","C","D",..: 1 1 1 1 1 1 1 1 1 1 ...</code></pre>
</div>

<div id="annex-b-confusion-matrices-for-random-forest-classifier">
<h1>
<a id="annex-b-confusion-matrices-for-random-forest-classifier" class="anchor" href="#annex-b-confusion-matrices-for-random-forest-classifier" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Annex B Confusion Matrices for Random Forest Classifier</h1>
<div id="insample-results">
<h2>
<a id="insample-results" class="anchor" href="#insample-results" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Insample Results</h2>
<pre><code>## Confusion Matrix and Statistics
## 
##           Reference
## Prediction    A    B    C    D    E
##          A 4185    0    0    0    0
##          B    2 2845    1    0    0
##          C    0    0 2567    0    0
##          D    0    0    0 2412    0
##          E    0    0    0    0 2706
## 
## Overall Statistics
##                                      
##                Accuracy : 0.9998     
##                  95% CI : (0.9994, 1)
##     No Information Rate : 0.2845     
##     P-Value [Acc &gt; NIR] : &lt; 2.2e-16  
##                                      
##                   Kappa : 0.9997     
##  Mcnemar's Test P-Value : NA         
## 
## Statistics by Class:
## 
##                      Class: A Class: B Class: C Class: D Class: E
## Sensitivity            0.9995   1.0000   0.9996   1.0000   1.0000
## Specificity            1.0000   0.9997   1.0000   1.0000   1.0000
## Pos Pred Value         1.0000   0.9989   1.0000   1.0000   1.0000
## Neg Pred Value         0.9998   1.0000   0.9999   1.0000   1.0000
## Prevalence             0.2845   0.1933   0.1745   0.1639   0.1839
## Detection Rate         0.2843   0.1933   0.1744   0.1639   0.1839
## Detection Prevalence   0.2843   0.1935   0.1744   0.1639   0.1839
## Balanced Accuracy      0.9998   0.9999   0.9998   1.0000   1.0000</code></pre>
</div>

<div id="out-of-sample-results">
<h2>
<a id="out-of-sample-results" class="anchor" href="#out-of-sample-results" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Out of Sample Results</h2>
<pre><code>## Confusion Matrix and Statistics
## 
##           Reference
## Prediction    A    B    C    D    E
##          A 1395    0    0    0    0
##          B    0  949    0    0    0
##          C    0    0  855    0    0
##          D    0    0    1  802    1
##          E    0    0    0    0  901
## 
## Overall Statistics
##                                      
##                Accuracy : 0.9996     
##                  95% CI : (0.9985, 1)
##     No Information Rate : 0.2845     
##     P-Value [Acc &gt; NIR] : &lt; 2.2e-16  
##                                      
##                   Kappa : 0.9995     
##  Mcnemar's Test P-Value : NA         
## 
## Statistics by Class:
## 
##                      Class: A Class: B Class: C Class: D Class: E
## Sensitivity            1.0000   1.0000   0.9988   1.0000   0.9989
## Specificity            1.0000   1.0000   1.0000   0.9995   1.0000
## Pos Pred Value         1.0000   1.0000   1.0000   0.9975   1.0000
## Neg Pred Value         1.0000   1.0000   0.9998   1.0000   0.9998
## Prevalence             0.2845   0.1935   0.1746   0.1635   0.1839
## Detection Rate         0.2845   0.1935   0.1743   0.1635   0.1837
## Detection Prevalence   0.2845   0.1935   0.1743   0.1639   0.1837
## Balanced Accuracy      1.0000   1.0000   0.9994   0.9998   0.9994</code></pre>
</div>

<p></p>
</div>

<div id="annex-c-confusion-matrices-for-boosting-classifier">
<h1>
<a id="annex-c-confusion-matrices-for-boosting-classifier" class="anchor" href="#annex-c-confusion-matrices-for-boosting-classifier" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Annex C Confusion Matrices for Boosting Classifier</h1>
<div id="insample-results-1">
<h2>
<a id="insample-results-1" class="anchor" href="#insample-results-1" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Insample Results</h2>
<pre><code>## Confusion Matrix and Statistics
## 
##           Reference
## Prediction    A    B    C    D    E
##          A 4179    6    0    0    0
##          B    9 2819   15    5    0
##          C    0   20 2545    1    1
##          D    1    4   30 2375    2
##          E    0    4    3   15 2684
## 
## Overall Statistics
##                                           
##                Accuracy : 0.9921          
##                  95% CI : (0.9906, 0.9935)
##     No Information Rate : 0.2846          
##     P-Value [Acc &gt; NIR] : &lt; 2.2e-16       
##                                           
##                   Kappa : 0.99            
##  Mcnemar's Test P-Value : NA              
## 
## Statistics by Class:
## 
##                      Class: A Class: B Class: C Class: D Class: E
## Sensitivity            0.9976   0.9881   0.9815   0.9912   0.9989
## Specificity            0.9994   0.9976   0.9982   0.9970   0.9982
## Pos Pred Value         0.9986   0.9898   0.9914   0.9847   0.9919
## Neg Pred Value         0.9991   0.9971   0.9960   0.9983   0.9998
## Prevalence             0.2846   0.1938   0.1762   0.1628   0.1826
## Detection Rate         0.2839   0.1915   0.1729   0.1614   0.1824
## Detection Prevalence   0.2843   0.1935   0.1744   0.1639   0.1839
## Balanced Accuracy      0.9985   0.9928   0.9898   0.9941   0.9985</code></pre>
</div>

<div id="out-of-sample-results-1">
<h2>
<a id="out-of-sample-results-1" class="anchor" href="#out-of-sample-results-1" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Out of Sample Results</h2>
<pre><code>## Confusion Matrix and Statistics
## 
##           Reference
## Prediction    A    B    C    D    E
##          A 1391    4    0    0    0
##          B    2  941    5    1    0
##          C    0    5  850    0    0
##          D    0    2    6  795    1
##          E    0    2    2    5  892
## 
## Overall Statistics
##                                          
##                Accuracy : 0.9929         
##                  95% CI : (0.9901, 0.995)
##     No Information Rate : 0.2841         
##     P-Value [Acc &gt; NIR] : &lt; 2.2e-16      
##                                          
##                   Kappa : 0.991          
##  Mcnemar's Test P-Value : NA             
## 
## Statistics by Class:
## 
##                      Class: A Class: B Class: C Class: D Class: E
## Sensitivity            0.9986   0.9864   0.9849   0.9925   0.9989
## Specificity            0.9989   0.9980   0.9988   0.9978   0.9978
## Pos Pred Value         0.9971   0.9916   0.9942   0.9888   0.9900
## Neg Pred Value         0.9994   0.9967   0.9968   0.9985   0.9998
## Prevalence             0.2841   0.1945   0.1760   0.1633   0.1821
## Detection Rate         0.2836   0.1919   0.1733   0.1621   0.1819
## Detection Prevalence   0.2845   0.1935   0.1743   0.1639   0.1837
## Balanced Accuracy      0.9987   0.9922   0.9918   0.9952   0.9983</code></pre>
</div>

<p></p>
</div>

<p></p>







<p></p>

      <footer class="site-footer">
        <span class="site-footer-owner"><a href="https://github.com/ermckinnon/github.io">Machine Learning Project</a> is maintained by <a href="https://github.com/ermckinnon">ermckinnon</a>.</span>

        <span class="site-footer-credits">This page was generated by <a href="https://pages.github.com">GitHub Pages</a> using the <a href="https://github.com/jasonlong/cayman-theme">Cayman theme</a> by <a href="https://twitter.com/jasonlong">Jason Long</a>.</span>
      </footer>

    </section>

  
  </body>
</html>
